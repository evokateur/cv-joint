job_analysis_task:
  description: >
    Analyze the job posting at {job_posting_url}.

    If a content file is provided ({content_file}), read the content from that local
    file instead of fetching from the URL. The URL is still the canonical source.

    Extract structured information into the JobPosting schema:

    **1. Metadata**
    - Extract job title, company name, industry, full description, and experience level.

    **2. Requirements**
    - Extract degrees, certifications, and explicit "years of experience" requirements.
    - Identify hard requirements (licenses, mandatory credentials).

    **3. Skills**
    - Technical skills: programming languages, frameworks, cloud platforms, tools.
    - Soft skills: communication, teamwork, leadership, problem-solving.
    - Preferred skills: "nice-to-have" extras.

    **4. Responsibilities**
    - Bullet out day-to-day tasks, focusing on specific duties (not vague culture blurbs).

    **5. ATS Alignment**
    - Keywords: important phrases used in the posting for filtering.
    - Tools/tech stack: specific named technologies (AWS, Docker, React, etc).

    **6. Normalization**
    - Deduplicate terms (merge "Python 3" and "Python").
    - Expand acronyms if possible (e.g., "CI/CD" -> "Continuous Integration / Continuous Deployment").
    - Filter out vague corporate jargon ("fast-paced environment", "great culture").

    Output must be a valid JobPosting object.
  expected_output: >
    A validated JobPosting object with all fields populated where possible,
    ensuring consistency and ATS-awareness.
  agent: job_analyst
